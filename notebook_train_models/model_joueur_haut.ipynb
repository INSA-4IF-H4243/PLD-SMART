{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle de réseaux de neurones sur le dataset du Joueur haut - Henri, Minh, Maxime\n",
    "- Coup droit\n",
    "- Déplacement\n",
    "- Revers\n",
    "- Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from smart.video import Video, Image\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras import optimizers, losses, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "path_dataset = 'img/JHaut'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Réseau de neurones classique"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prises des frames et des vidéos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_videos = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "all_paths = []\n",
    "for dirpath, dirnames, _ in os.walk(path_dataset):\n",
    "    for dir_type in dirnames:\n",
    "        \n",
    "        path_fol = os.path.join(dirpath, dir_type)\n",
    "        for file in os.listdir(path_fol):\n",
    "            path_fol_img = os.path.join(path_fol, file)\n",
    "            if os.path.isdir(path_fol_img) and file == 'images':\n",
    "                frames = []\n",
    "                for file_img in os.listdir(path_fol_img):\n",
    "                    path_img = os.path.join(path_fol_img, file_img)\n",
    "                    img_obj = Image.load_image(cv2.IMREAD_GRAYSCALE, path_img)\n",
    "                    img = img_obj.img\n",
    "                    frames.append(img)\t\n",
    "                if (len(frames) == 15):\n",
    "                    output_res = path_img.split('\\\\')[1]\n",
    "                    all_paths.append(path_fol_img)\n",
    "                    vid = Video.read_video_from_frames(frames)\n",
    "                    list_videos.append(vid)\n",
    "                    y.append(output_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<smart.video.Video.Video at 0x1c66105e8e0>,\n",
       " <smart.video.Video.Video at 0x1c66141e370>,\n",
       " <smart.video.Video.Video at 0x1c66141e2b0>,\n",
       " <smart.video.Video.Video at 0x1c66141e2e0>,\n",
       " <smart.video.Video.Video at 0x1c66141e460>,\n",
       " <smart.video.Video.Video at 0x1c66141e4c0>,\n",
       " <smart.video.Video.Video at 0x1c66141e070>,\n",
       " <smart.video.Video.Video at 0x1c66141e580>,\n",
       " <smart.video.Video.Video at 0x1c66141e400>,\n",
       " <smart.video.Video.Video at 0x1c66141e5b0>,\n",
       " <smart.video.Video.Video at 0x1c66141e640>,\n",
       " <smart.video.Video.Video at 0x1c66141e6a0>,\n",
       " <smart.video.Video.Video at 0x1c66141e700>,\n",
       " <smart.video.Video.Video at 0x1c66141e850>,\n",
       " <smart.video.Video.Video at 0x1c66141e7f0>,\n",
       " <smart.video.Video.Video at 0x1c66141e790>,\n",
       " <smart.video.Video.Video at 0x1c66141e130>,\n",
       " <smart.video.Video.Video at 0x1c66141e3d0>,\n",
       " <smart.video.Video.Video at 0x1c66141e100>,\n",
       " <smart.video.Video.Video at 0x1c66141e1f0>,\n",
       " <smart.video.Video.Video at 0x1c6612c8d00>,\n",
       " <smart.video.Video.Video at 0x1c6612c8dc0>,\n",
       " <smart.video.Video.Video at 0x1c660ea94f0>,\n",
       " <smart.video.Video.Video at 0x1c66141e0a0>,\n",
       " <smart.video.Video.Video at 0x1c6612df9d0>,\n",
       " <smart.video.Video.Video at 0x1c6612dfc40>,\n",
       " <smart.video.Video.Video at 0x1c6612df940>,\n",
       " <smart.video.Video.Video at 0x1c660ea9610>,\n",
       " <smart.video.Video.Video at 0x1c660ea5490>,\n",
       " <smart.video.Video.Video at 0x1c660ea5d30>,\n",
       " <smart.video.Video.Video at 0x1c660ea5220>,\n",
       " <smart.video.Video.Video at 0x1c6612dfc70>,\n",
       " <smart.video.Video.Video at 0x1c660ea5100>,\n",
       " <smart.video.Video.Video at 0x1c660ea5eb0>,\n",
       " <smart.video.Video.Video at 0x1c66140ebe0>,\n",
       " <smart.video.Video.Video at 0x1c66140e1c0>,\n",
       " <smart.video.Video.Video at 0x1c66103c7c0>,\n",
       " <smart.video.Video.Video at 0x1c661413640>,\n",
       " <smart.video.Video.Video at 0x1c661413a30>,\n",
       " <smart.video.Video.Video at 0x1c661413f40>,\n",
       " <smart.video.Video.Video at 0x1c661413d00>,\n",
       " <smart.video.Video.Video at 0x1c661413e50>,\n",
       " <smart.video.Video.Video at 0x1c661413ca0>,\n",
       " <smart.video.Video.Video at 0x1c661413370>,\n",
       " <smart.video.Video.Video at 0x1c661413190>,\n",
       " <smart.video.Video.Video at 0x1c6614133a0>,\n",
       " <smart.video.Video.Video at 0x1c661413880>,\n",
       " <smart.video.Video.Video at 0x1c661413250>,\n",
       " <smart.video.Video.Video at 0x1c661413f10>,\n",
       " <smart.video.Video.Video at 0x1c661413460>,\n",
       " <smart.video.Video.Video at 0x1c661041ee0>,\n",
       " <smart.video.Video.Video at 0x1c661413490>,\n",
       " <smart.video.Video.Video at 0x1c66141e280>,\n",
       " <smart.video.Video.Video at 0x1c661055100>,\n",
       " <smart.video.Video.Video at 0x1c66141e190>,\n",
       " <smart.video.Video.Video at 0x1c6612d2190>,\n",
       " <smart.video.Video.Video at 0x1c661413340>,\n",
       " <smart.video.Video.Video at 0x1c6614280a0>,\n",
       " <smart.video.Video.Video at 0x1c661428100>,\n",
       " <smart.video.Video.Video at 0x1c661428160>,\n",
       " <smart.video.Video.Video at 0x1c6614281c0>,\n",
       " <smart.video.Video.Video at 0x1c661428220>,\n",
       " <smart.video.Video.Video at 0x1c661428280>,\n",
       " <smart.video.Video.Video at 0x1c6614282e0>,\n",
       " <smart.video.Video.Video at 0x1c661428340>,\n",
       " <smart.video.Video.Video at 0x1c661428430>,\n",
       " <smart.video.Video.Video at 0x1c661428040>]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coup droit',\n",
       " 'coup droit',\n",
       " 'coup droit',\n",
       " 'coup droit',\n",
       " 'coup droit',\n",
       " 'coup droit',\n",
       " 'coup droit',\n",
       " 'coup droit',\n",
       " 'coup droit',\n",
       " 'coup droit',\n",
       " 'coup droit',\n",
       " 'coup droit',\n",
       " 'coup droit',\n",
       " 'coup droit',\n",
       " 'coup droit',\n",
       " 'coup droit',\n",
       " 'deplacement',\n",
       " 'deplacement',\n",
       " 'deplacement',\n",
       " 'deplacement',\n",
       " 'deplacement',\n",
       " 'deplacement',\n",
       " 'deplacement',\n",
       " 'deplacement',\n",
       " 'deplacement',\n",
       " 'deplacement',\n",
       " 'deplacement',\n",
       " 'deplacement',\n",
       " 'deplacement',\n",
       " 'deplacement',\n",
       " 'deplacement',\n",
       " 'deplacement',\n",
       " 'deplacement',\n",
       " 'deplacement',\n",
       " 'deplacement',\n",
       " 'deplacement',\n",
       " 'deplacement',\n",
       " 'deplacement',\n",
       " 'deplacement',\n",
       " 'deplacement',\n",
       " 'revers',\n",
       " 'revers',\n",
       " 'revers',\n",
       " 'revers',\n",
       " 'revers',\n",
       " 'revers',\n",
       " 'revers',\n",
       " 'revers',\n",
       " 'revers',\n",
       " 'revers',\n",
       " 'revers',\n",
       " 'revers',\n",
       " 'revers',\n",
       " 'revers',\n",
       " 'service',\n",
       " 'service',\n",
       " 'service',\n",
       " 'service',\n",
       " 'service',\n",
       " 'service',\n",
       " 'service',\n",
       " 'service',\n",
       " 'service',\n",
       " 'service',\n",
       " 'service',\n",
       " 'service',\n",
       " 'service']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output sans label encoder\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "encoder = LabelEncoder()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer output y en one hot vector\n",
    "- 0: coup droit\n",
    "- 1: déplacement\n",
    "- 2: revers\n",
    "- 3: service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3], dtype=int64)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = encoder.fit_transform(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_y = np.unique(y)\n",
    "output_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37500"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Résolution des images\n",
    "input_shape_model = list_videos[0].frames.shape[0] * list_videos[0].frames.shape[1] * list_videos[0].frames.shape[2]\n",
    "input_shape_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(list_videos) * 0.8)\n",
    "test_size = len(list_videos) - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 53, 14, 14)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Split video train, test\n",
    "random.seed(42)\n",
    "vids_train = random.sample(list_videos, k = train_size)\n",
    "vids_test = [vid for vid in list_videos if vid not in vids_train]\n",
    "y_train = [y[i] for i in range(len(y)) if list_videos[i] in vids_train]\n",
    "y_test = [y[i] for i in range(len(y)) if list_videos[i] in vids_test]\n",
    "paths_test = [all_paths[i] for i in range(len(all_paths)) if list_videos[i] in vids_test]\n",
    "len(vids_train), len(y_train), len(vids_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['img/JHaut\\\\coup droit\\\\AS_18386\\\\images',\n",
       " 'img/JHaut\\\\coup droit\\\\AS_29198\\\\images',\n",
       " 'img/JHaut\\\\coup droit\\\\car5493-5847138\\\\images',\n",
       " 'img/JHaut\\\\deplacement\\\\17694-1786278\\\\images',\n",
       " 'img/JHaut\\\\deplacement\\\\1980-2563441\\\\images',\n",
       " 'img/JHaut\\\\deplacement\\\\28040-28230129\\\\images',\n",
       " 'img/JHaut\\\\deplacement\\\\44260-4450099\\\\images',\n",
       " 'img/JHaut\\\\deplacement\\\\6481-660086\\\\images',\n",
       " 'img/JHaut\\\\deplacement\\\\AS_15180\\\\images',\n",
       " 'img/JHaut\\\\revers\\\\AS_34270\\\\images',\n",
       " 'img/JHaut\\\\revers\\\\car5936-6390142\\\\images',\n",
       " 'img/JHaut\\\\revers\\\\car5936-6390257\\\\images',\n",
       " 'img/JHaut\\\\service\\\\AS_19185\\\\images',\n",
       " 'img/JHaut\\\\service\\\\AS_35549\\\\images']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.array(y_train, dtype=int)\n",
    "y_test = np.array(y_test, dtype=int)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 37500)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split input X en train, test set\n",
    "X_train = np.zeros((train_size, 50*50*15), dtype=int)\n",
    "for i, vid in enumerate(vids_train):\n",
    "    seq_img = vid.frames.flatten()\n",
    "    X_train[i] = seq_img\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.zeros((test_size, 50*50*15), dtype=int)\n",
    "for i, vid in enumerate(vids_test):\n",
    "    seq_img = vid.frames.flatten()\n",
    "    X_test[i] = seq_img\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle de réseau de neurones avec classification multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_23 (Dense)            (None, 16)                600016    \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 32)                544       \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 611,508\n",
      "Trainable params: 611,508\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1234) # for consistent results\n",
    "model = Sequential(\n",
    "    [               \n",
    "        tf.keras.Input(shape = input_shape_model), # 50*50 * 15 = 37500\n",
    "        Dense(units=16, activation=\"relu\"),\n",
    "        Dense(units=32, activation=\"relu\"),\n",
    "        Dense(units=64, activation=\"sigmoid\"),\n",
    "        Dense(units=128, activation=\"relu\"),\n",
    "        Dense(units=len(output_y), activation=\"softmax\"),\n",
    "    ]\n",
    ")          \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = 'checkpoints/Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.6034 - sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 1: val_loss improved from inf to 1.63703, saving model to checkpoints\\Weights-001--1.63703.hdf5\n",
      "2/2 [==============================] - 1s 461ms/step - loss: 1.5686 - sparse_categorical_accuracy: 0.0714 - val_loss: 1.6370 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.3144 - sparse_categorical_accuracy: 0.2812\n",
      "Epoch 2: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 1.2922 - sparse_categorical_accuracy: 0.3333 - val_loss: 2.2306 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 3/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.1354 - sparse_categorical_accuracy: 0.3750\n",
      "Epoch 3: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 1.1150 - sparse_categorical_accuracy: 0.4524 - val_loss: 2.7827 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 4/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.0569 - sparse_categorical_accuracy: 0.4375\n",
      "Epoch 4: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 1.0494 - sparse_categorical_accuracy: 0.4286 - val_loss: 3.2537 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 5/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.9936 - sparse_categorical_accuracy: 0.4375\n",
      "Epoch 5: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 1.0004 - sparse_categorical_accuracy: 0.4286 - val_loss: 3.5770 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 6/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.9524 - sparse_categorical_accuracy: 0.4062\n",
      "Epoch 6: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.9171 - sparse_categorical_accuracy: 0.4524 - val_loss: 3.7833 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 7/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.8815 - sparse_categorical_accuracy: 0.4062\n",
      "Epoch 7: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.8273 - sparse_categorical_accuracy: 0.5238 - val_loss: 3.9725 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 8/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.7386 - sparse_categorical_accuracy: 0.7812\n",
      "Epoch 8: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.7339 - sparse_categorical_accuracy: 0.8095 - val_loss: 4.1892 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 9/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6562 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 9: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.6513 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.4151 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 10/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5853 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 10: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 0.5687 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.6289 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 11/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.4882 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 11: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.4798 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.7970 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 12/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.4051 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 12: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.3942 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.9176 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 13/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.3276 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 13: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3208 - sparse_categorical_accuracy: 1.0000 - val_loss: 5.0186 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 14/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2742 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 14: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.2618 - sparse_categorical_accuracy: 1.0000 - val_loss: 5.1454 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 15/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2076 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 15: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 0.2079 - sparse_categorical_accuracy: 1.0000 - val_loss: 5.3337 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 16/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1652 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 16: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.1608 - sparse_categorical_accuracy: 1.0000 - val_loss: 5.5703 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 17/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1281 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 17: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.1260 - sparse_categorical_accuracy: 1.0000 - val_loss: 5.8112 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 18/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0986 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 18: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.1013 - sparse_categorical_accuracy: 1.0000 - val_loss: 5.9836 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 19/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0846 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 19: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.0803 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.0546 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 20/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0650 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 20: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0628 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.0958 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 21/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0507 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 21: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.0499 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.1394 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 22/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0426 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 22: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.0404 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.1909 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 23/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0344 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 23: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0332 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.2467 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 24/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0293 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 24: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.0279 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.3105 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 25/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0236 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 25: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0237 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.3818 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 26/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0207 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 26: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.0205 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.4599 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 27/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0186 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 27: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.0178 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.5424 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 28/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0161 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 28: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0157 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.6213 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 29/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0145 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 29: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.0139 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.6970 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 30/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0129 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 30: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.0124 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.7678 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 31/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0117 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 31: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.0112 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.8269 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 32/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0101 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 32: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.0102 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.8805 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 33/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0094 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 33: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0093 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.9340 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 34/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0086 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 34: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.0086 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.9854 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 35/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0080 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 35: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.0080 - sparse_categorical_accuracy: 1.0000 - val_loss: 7.0330 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 36/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0072 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 36: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0074 - sparse_categorical_accuracy: 1.0000 - val_loss: 7.0783 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 37/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0070 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 37: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0069 - sparse_categorical_accuracy: 1.0000 - val_loss: 7.1209 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 38/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0066 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 38: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0065 - sparse_categorical_accuracy: 1.0000 - val_loss: 7.1591 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 39/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0061 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 39: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 0.0061 - sparse_categorical_accuracy: 1.0000 - val_loss: 7.1968 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 40/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0058 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 40: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.0057 - sparse_categorical_accuracy: 1.0000 - val_loss: 7.2356 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 41/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0053 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 41: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.0054 - sparse_categorical_accuracy: 1.0000 - val_loss: 7.2726 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 42/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0050 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 42: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 0.0050 - sparse_categorical_accuracy: 1.0000 - val_loss: 7.3091 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 43/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0048 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 43: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.0047 - sparse_categorical_accuracy: 1.0000 - val_loss: 7.3508 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 44/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0045 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 44: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.0045 - sparse_categorical_accuracy: 1.0000 - val_loss: 7.3946 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 45/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0043 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 45: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.0042 - sparse_categorical_accuracy: 1.0000 - val_loss: 7.4353 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 46/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0040 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 46: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.0040 - sparse_categorical_accuracy: 1.0000 - val_loss: 7.4722 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 47/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0040 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 47: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0039 - sparse_categorical_accuracy: 1.0000 - val_loss: 7.5062 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 48/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0037 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 48: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.0037 - sparse_categorical_accuracy: 1.0000 - val_loss: 7.5377 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 49/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0034 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 49: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.0035 - sparse_categorical_accuracy: 1.0000 - val_loss: 7.5675 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 50/50\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0035 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 50: val_loss did not improve from 1.63703\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.0034 - sparse_categorical_accuracy: 1.0000 - val_loss: 7.5948 - val_sparse_categorical_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer = optimizers.Adam(learning_rate=0.001),\n",
    "    loss = losses.SparseCategoricalCrossentropy(),\n",
    "    metrics = [metrics.SparseCategoricalAccuracy()]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs = 50,\n",
    "    # batch_size = 32, # données transmises pour une session\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step - loss: 2.1533 - sparse_categorical_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.153291940689087, 0.5]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.3768126e-01, 5.8769848e-02, 3.3202462e-03, 2.2866404e-04],\n",
       "       [1.4789995e-02, 9.7630125e-01, 8.7563572e-03, 1.5232492e-04],\n",
       "       [4.2683479e-01, 5.6289965e-01, 9.7612320e-03, 5.0437305e-04],\n",
       "       [3.4849536e-02, 8.5830021e-01, 1.0647215e-01, 3.7820748e-04],\n",
       "       [2.6128751e-01, 7.3313445e-01, 5.2384809e-03, 3.3956187e-04],\n",
       "       [1.3584740e-02, 9.7620732e-01, 1.0067902e-02, 1.4004315e-04],\n",
       "       [6.7455277e-02, 9.2993563e-01, 2.4244040e-03, 1.8471872e-04],\n",
       "       [1.5410262e-02, 9.8336315e-01, 1.1361671e-03, 9.0343259e-05],\n",
       "       [9.8351246e-01, 1.5533330e-02, 8.4053824e-04, 1.1367043e-04],\n",
       "       [2.5411665e-02, 9.5028055e-01, 2.4057344e-02, 2.5041259e-04],\n",
       "       [5.9056103e-02, 9.2747504e-01, 1.3100634e-02, 3.6827565e-04],\n",
       "       [4.2315223e-03, 1.8263814e-01, 8.1289792e-01, 2.3245366e-04],\n",
       "       [4.4910014e-03, 9.9560238e-02, 8.9574826e-01, 2.0048725e-04],\n",
       "       [7.4567511e-03, 8.4417129e-01, 1.4810446e-01, 2.6744147e-04]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(X_test)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels:  [0 1 1 1 1 1 1 1 0 1 1 2 2 1]\n",
      "True labels:       [0 0 0 1 1 1 1 1 1 2 2 2 3 3]\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(pred, axis=1)\n",
    "print(\"Predicted labels: \", y_pred)\n",
    "print(\"True labels:      \", y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy: $7/14$ ($50 \\%$)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Réseaux de neurones convolutifs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prises des frames et des vidéos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_videos_cnn = []\n",
    "y_cnn = []\n",
    "for dirpath, dirnames, _ in os.walk(path_dataset):\n",
    "    for dir_type in dirnames:\n",
    "        \n",
    "        path_fol = os.path.join(dirpath, dir_type)\n",
    "        for file in os.listdir(path_fol):\n",
    "            path_fol_img = os.path.join(path_fol, file)\n",
    "            if os.path.isdir(path_fol_img) and file == 'images':\n",
    "                frames = []\n",
    "                for file_img in os.listdir(path_fol_img):\n",
    "                    path_img = os.path.join(path_fol_img, file_img)\n",
    "                    img_obj = Image.load_image(cv2.IMREAD_COLOR, path_img)\n",
    "                    img = img_obj.img\n",
    "                    frames.append(img)\t\n",
    "                if (len(frames) == 15):\n",
    "                    output_res = path_img.split('\\\\')[1]\n",
    "                    vid = Video.read_video_from_frames(frames)\n",
    "                    list_videos_cnn.append(vid)\n",
    "                    y_cnn.append(output_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50, 3)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_videos_cnn[0].frames.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "encoder = LabelEncoder()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer output y en one hot vector\n",
    "- 0: coup droit\n",
    "- 1: déplacement\n",
    "- 2: revers\n",
    "- 3: service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3], dtype=int64)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_cnn = encoder.fit_transform(y_cnn)\n",
    "y_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_y = np.unique(y_cnn)\n",
    "output_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_videos_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(list_videos_cnn) * 0.8)\n",
    "test_size = len(list_videos_cnn) - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 53, 14, 14)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Split video train, test\n",
    "random.seed(42)\n",
    "vids_train_cnn = random.sample(list_videos_cnn, k = train_size)\n",
    "vids_test_cnn = [vid for vid in list_videos_cnn if vid not in vids_train_cnn]\n",
    "y_train_cnn = [y_cnn[i] for i in range(len(y_cnn)) if list_videos_cnn[i] in vids_train_cnn]\n",
    "y_test_cnn = [y_cnn[i] for i in range(len(y_cnn)) if list_videos_cnn[i] in vids_test_cnn]\n",
    "len(vids_train_cnn), len(y_train_cnn), len(vids_test_cnn), len(y_test_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_cnn = np.array(y_train_cnn, dtype=int)\n",
    "y_test_cnn = np.array(y_test_cnn, dtype=int)\n",
    "y_train_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 50, 750, 3)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split input X en train, test set\n",
    "X_train_cnn = np.zeros((train_size, 50, 15*50, 3), dtype=int)\n",
    "for i, vid in enumerate(vids_train_cnn):\n",
    "    for j, frame in enumerate(vid.frames):\n",
    "        X_train_cnn[i, :, 50*j:50*(j+1), :] = frame\n",
    "X_train_cnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 50, 750, 3)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_cnn = np.zeros((test_size, 50, 15*50, 3), dtype=int)\n",
    "for i, vid in enumerate(vids_test_cnn):\n",
    "    for j, frame in enumerate(vid.frames):\n",
    "        X_test_cnn[i, :, 50*j:50*(j+1), :] = frame\n",
    "X_test_cnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 750, 3)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape_cnn = X_train_cnn.shape[1:]\n",
    "input_shape_cnn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle de réseau de neurones convolutifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 48, 748, 16)       448       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 48, 748, 16)      64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 24, 374, 16)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 24, 374, 16)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 22, 372, 32)       4640      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 22, 372, 32)      128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 11, 186, 32)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 11, 186, 32)       0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 65472)             0         \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 4)                 261892    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 267,172\n",
      "Trainable params: 267,076\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_cnn = Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape = input_shape_cnn), # 50, 15*50, 3\n",
    "        Conv2D(filters=16, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Conv2D(filters=64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        # BatchNormalization(),\n",
    "        # MaxPooling2D(pool_size=(2, 2)),\n",
    "        # Dropout(0.2),\n",
    "        \n",
    "        Flatten(),\n",
    "        # Dense(units=128, activation=\"relu\"),\n",
    "        Dense(units=len(output_y), activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = 'checkpoints/Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "2/2 [==============================] - ETA: 0s - loss: 8.7628 - sparse_categorical_accuracy: 0.2381\n",
      "Epoch 1: val_loss improved from inf to 321.34100, saving model to checkpoints\\Weights-001--321.34100.hdf5\n",
      "2/2 [==============================] - 2s 556ms/step - loss: 8.7628 - sparse_categorical_accuracy: 0.2381 - val_loss: 321.3410 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 2/16\n",
      "2/2 [==============================] - ETA: 0s - loss: 8.6578 - sparse_categorical_accuracy: 0.6429 \n",
      "Epoch 2: val_loss improved from 321.34100 to 305.39270, saving model to checkpoints\\Weights-002--305.39270.hdf5\n",
      "2/2 [==============================] - 1s 379ms/step - loss: 8.6578 - sparse_categorical_accuracy: 0.6429 - val_loss: 305.3927 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 3/16\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.8751 - sparse_categorical_accuracy: 0.8333\n",
      "Epoch 3: val_loss improved from 305.39270 to 300.44040, saving model to checkpoints\\Weights-003--300.44040.hdf5\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.8751 - sparse_categorical_accuracy: 0.8333 - val_loss: 300.4404 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 4/16\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.5967 - sparse_categorical_accuracy: 0.8810\n",
      "Epoch 4: val_loss improved from 300.44040 to 286.19641, saving model to checkpoints\\Weights-004--286.19641.hdf5\n",
      "2/2 [==============================] - 1s 377ms/step - loss: 0.5967 - sparse_categorical_accuracy: 0.8810 - val_loss: 286.1964 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 5/16\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2548 - sparse_categorical_accuracy: 0.9048\n",
      "Epoch 5: val_loss improved from 286.19641 to 266.69699, saving model to checkpoints\\Weights-005--266.69699.hdf5\n",
      "2/2 [==============================] - 1s 317ms/step - loss: 0.2548 - sparse_categorical_accuracy: 0.9048 - val_loss: 266.6970 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 6/16\n",
      "2/2 [==============================] - ETA: 0s - loss: 9.2907e-05 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 6: val_loss improved from 266.69699 to 251.89609, saving model to checkpoints\\Weights-006--251.89609.hdf5\n",
      "2/2 [==============================] - 1s 320ms/step - loss: 9.2907e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 251.8961 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 7/16\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0020 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 7: val_loss improved from 251.89609 to 244.87389, saving model to checkpoints\\Weights-007--244.87389.hdf5\n",
      "2/2 [==============================] - 1s 330ms/step - loss: 0.0020 - sparse_categorical_accuracy: 1.0000 - val_loss: 244.8739 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 8/16\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.1696 - sparse_categorical_accuracy: 0.9524\n",
      "Epoch 8: val_loss improved from 244.87389 to 236.83701, saving model to checkpoints\\Weights-008--236.83701.hdf5\n",
      "2/2 [==============================] - 1s 292ms/step - loss: 0.1696 - sparse_categorical_accuracy: 0.9524 - val_loss: 236.8370 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 9/16\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.1714 - sparse_categorical_accuracy: 0.9524\n",
      "Epoch 9: val_loss improved from 236.83701 to 222.72020, saving model to checkpoints\\Weights-009--222.72020.hdf5\n",
      "2/2 [==============================] - 1s 348ms/step - loss: 0.1714 - sparse_categorical_accuracy: 0.9524 - val_loss: 222.7202 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 10/16\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0010 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 10: val_loss improved from 222.72020 to 211.38605, saving model to checkpoints\\Weights-010--211.38605.hdf5\n",
      "2/2 [==============================] - 1s 368ms/step - loss: 0.0010 - sparse_categorical_accuracy: 1.0000 - val_loss: 211.3860 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 11/16\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0422 - sparse_categorical_accuracy: 0.9762\n",
      "Epoch 11: val_loss improved from 211.38605 to 202.60454, saving model to checkpoints\\Weights-011--202.60454.hdf5\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.0422 - sparse_categorical_accuracy: 0.9762 - val_loss: 202.6045 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 12/16\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0049 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 12: val_loss improved from 202.60454 to 194.96449, saving model to checkpoints\\Weights-012--194.96449.hdf5\n",
      "2/2 [==============================] - 1s 363ms/step - loss: 0.0049 - sparse_categorical_accuracy: 1.0000 - val_loss: 194.9645 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 13/16\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.8902e-06 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 13: val_loss improved from 194.96449 to 188.74301, saving model to checkpoints\\Weights-013--188.74301.hdf5\n",
      "2/2 [==============================] - 1s 383ms/step - loss: 4.8902e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 188.7430 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 14/16\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2185e-06 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 14: val_loss improved from 188.74301 to 183.06476, saving model to checkpoints\\Weights-014--183.06476.hdf5\n",
      "2/2 [==============================] - 1s 342ms/step - loss: 3.2185e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 183.0648 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 15/16\n",
      "2/2 [==============================] - ETA: 0s - loss: 6.6499e-06 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 15: val_loss improved from 183.06476 to 178.26031, saving model to checkpoints\\Weights-015--178.26031.hdf5\n",
      "2/2 [==============================] - 1s 377ms/step - loss: 6.6499e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 178.2603 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 16/16\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1067e-05 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 16: val_loss improved from 178.26031 to 173.83955, saving model to checkpoints\\Weights-016--173.83955.hdf5\n",
      "2/2 [==============================] - 1s 373ms/step - loss: 1.1067e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 173.8396 - val_sparse_categorical_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "model_cnn.compile(\n",
    "    optimizer = optimizers.Adam(learning_rate=0.001),\n",
    "    loss = losses.SparseCategoricalCrossentropy(),\n",
    "    metrics = [metrics.SparseCategoricalAccuracy()]\n",
    ")\n",
    "\n",
    "history_cnn = model_cnn.fit(\n",
    "    X_train_cnn, y_train_cnn,\n",
    "    epochs = 16,\n",
    "    # batch_size = 32, # données transmises pour une session\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 98ms/step - loss: 25.9882 - sparse_categorical_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[25.988161087036133, 0.5]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cnn.evaluate(X_test_cnn, y_test_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 185ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.99857664e-01, 1.42352015e-04, 2.61616705e-12, 0.00000000e+00],\n",
       "       [7.20749767e-07, 9.99998093e-01, 1.19271272e-06, 0.00000000e+00],\n",
       "       [1.12818815e-02, 9.88718152e-01, 4.17848367e-10, 0.00000000e+00],\n",
       "       [1.54105457e-03, 9.98456359e-01, 2.59250919e-06, 0.00000000e+00],\n",
       "       [1.00000000e+00, 9.82029041e-11, 1.52353953e-14, 0.00000000e+00],\n",
       "       [2.17796094e-03, 9.97822046e-01, 4.54582505e-08, 0.00000000e+00],\n",
       "       [3.50141406e-01, 6.49858594e-01, 1.65433107e-14, 0.00000000e+00],\n",
       "       [2.63844339e-13, 1.00000000e+00, 4.54501700e-28, 0.00000000e+00],\n",
       "       [9.05383110e-01, 9.46169049e-02, 4.25474822e-10, 0.00000000e+00],\n",
       "       [2.71858624e-03, 4.84039390e-07, 9.97280955e-01, 0.00000000e+00],\n",
       "       [9.14212317e-11, 1.00000000e+00, 5.25618195e-11, 0.00000000e+00],\n",
       "       [2.47802943e-01, 3.17178220e-01, 4.35018867e-01, 0.00000000e+00],\n",
       "       [6.89164503e-10, 9.99999881e-01, 1.76571220e-07, 0.00000000e+00],\n",
       "       [1.87707939e-11, 1.00000000e+00, 1.74362880e-09, 0.00000000e+00]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_cnn = model_cnn.predict(X_test_cnn)\n",
    "pred_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels:  [0 1 1 1 0 1 1 1 0 2 1 2 1 1]\n",
      "True labels:       [0 0 0 1 1 1 1 1 1 2 2 2 3 3]\n"
     ]
    }
   ],
   "source": [
    "y_pred_cnn = np.argmax(pred_cnn, axis=1)\n",
    "print(\"Predicted labels: \", y_pred_cnn)\n",
    "print(\"True labels:      \", y_test_cnn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy: $9 / 14$ ($64.28 \\%$)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
