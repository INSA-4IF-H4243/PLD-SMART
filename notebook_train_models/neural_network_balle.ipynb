{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "from keras import regularizers\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1_0</th>\n",
       "      <th>col1_1</th>\n",
       "      <th>col2_0</th>\n",
       "      <th>col2_1</th>\n",
       "      <th>col3_0</th>\n",
       "      <th>col3_1</th>\n",
       "      <th>col4_0</th>\n",
       "      <th>col4_1</th>\n",
       "      <th>col5_0</th>\n",
       "      <th>col5_1</th>\n",
       "      <th>...</th>\n",
       "      <th>col41_1</th>\n",
       "      <th>col42_0</th>\n",
       "      <th>col42_1</th>\n",
       "      <th>col43_0</th>\n",
       "      <th>col43_1</th>\n",
       "      <th>col44_0</th>\n",
       "      <th>col44_1</th>\n",
       "      <th>col45_0</th>\n",
       "      <th>col45_1</th>\n",
       "      <th>col46</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>175.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>...</td>\n",
       "      <td>119.0</td>\n",
       "      <td>374.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>176.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>...</td>\n",
       "      <td>121.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>374.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>383.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>382.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>176.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>...</td>\n",
       "      <td>113.0</td>\n",
       "      <td>375.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>378.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>383.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>176.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>...</td>\n",
       "      <td>120.0</td>\n",
       "      <td>368.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>384.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>386.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>176.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>...</td>\n",
       "      <td>113.0</td>\n",
       "      <td>374.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>373.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>392.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   col1_0  col1_1  col2_0  col2_1  col3_0  col3_1  col4_0  col4_1  col5_0  \\\n",
       "0   175.0   232.0   184.0   227.0   195.0   229.0   200.0   215.0   196.0   \n",
       "1   176.0   232.0   191.0   229.0   188.0   233.0   200.0   216.0   205.0   \n",
       "2   176.0   236.0   191.0   226.0   196.0   227.0   200.0   221.0   199.0   \n",
       "3   176.0   238.0   184.0   236.0   191.0   224.0   189.0   214.0   194.0   \n",
       "4   176.0   242.0   185.0   231.0   191.0   228.0   187.0   224.0   195.0   \n",
       "\n",
       "   col5_1  ...  col41_1  col42_0  col42_1  col43_0  col43_1  col44_0  col44_1  \\\n",
       "0   213.0  ...    119.0    374.0    114.0    371.0    110.0    380.0    108.0   \n",
       "1   217.0  ...    121.0    377.0    110.0    374.0    107.0    383.0     97.0   \n",
       "2   221.0  ...    113.0    375.0    106.0    377.0    110.0    378.0     97.0   \n",
       "3   215.0  ...    120.0    368.0    105.0    380.0    108.0    384.0     96.0   \n",
       "4   219.0  ...    113.0    374.0    105.0    373.0    109.0    380.0     98.0   \n",
       "\n",
       "   col45_0  col45_1  col46  \n",
       "0    391.0     93.0    2.0  \n",
       "1    382.0    101.0    2.0  \n",
       "2    383.0     95.0    2.0  \n",
       "3    386.0     94.0    2.0  \n",
       "4    392.0    102.0    2.0  \n",
       "\n",
       "[5 rows x 91 columns]"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset/new_pos.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1_0</th>\n",
       "      <th>col1_1</th>\n",
       "      <th>col2_0</th>\n",
       "      <th>col2_1</th>\n",
       "      <th>col3_0</th>\n",
       "      <th>col3_1</th>\n",
       "      <th>col4_0</th>\n",
       "      <th>col4_1</th>\n",
       "      <th>col5_0</th>\n",
       "      <th>col5_1</th>\n",
       "      <th>...</th>\n",
       "      <th>col41_1</th>\n",
       "      <th>col42_0</th>\n",
       "      <th>col42_1</th>\n",
       "      <th>col43_0</th>\n",
       "      <th>col43_1</th>\n",
       "      <th>col44_0</th>\n",
       "      <th>col44_1</th>\n",
       "      <th>col45_0</th>\n",
       "      <th>col45_1</th>\n",
       "      <th>col46</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6463.000000</td>\n",
       "      <td>6463.000000</td>\n",
       "      <td>6463.000000</td>\n",
       "      <td>6463.000000</td>\n",
       "      <td>6463.000000</td>\n",
       "      <td>6463.000000</td>\n",
       "      <td>6463.000000</td>\n",
       "      <td>6463.000000</td>\n",
       "      <td>6463.000000</td>\n",
       "      <td>6463.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6463.000000</td>\n",
       "      <td>6463.000000</td>\n",
       "      <td>6463.000000</td>\n",
       "      <td>6463.000000</td>\n",
       "      <td>6463.000000</td>\n",
       "      <td>6463.000000</td>\n",
       "      <td>6463.000000</td>\n",
       "      <td>6463.000000</td>\n",
       "      <td>6463.000000</td>\n",
       "      <td>6463.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>301.635928</td>\n",
       "      <td>172.079375</td>\n",
       "      <td>301.376141</td>\n",
       "      <td>169.084326</td>\n",
       "      <td>301.932230</td>\n",
       "      <td>166.213523</td>\n",
       "      <td>301.843571</td>\n",
       "      <td>163.366084</td>\n",
       "      <td>301.711898</td>\n",
       "      <td>160.375058</td>\n",
       "      <td>...</td>\n",
       "      <td>155.641343</td>\n",
       "      <td>299.240755</td>\n",
       "      <td>155.738357</td>\n",
       "      <td>299.663160</td>\n",
       "      <td>156.079684</td>\n",
       "      <td>299.417763</td>\n",
       "      <td>156.613337</td>\n",
       "      <td>299.710351</td>\n",
       "      <td>157.583475</td>\n",
       "      <td>6.844499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>75.307941</td>\n",
       "      <td>70.621681</td>\n",
       "      <td>74.091861</td>\n",
       "      <td>67.475299</td>\n",
       "      <td>72.442872</td>\n",
       "      <td>64.807385</td>\n",
       "      <td>71.027193</td>\n",
       "      <td>62.238422</td>\n",
       "      <td>69.215780</td>\n",
       "      <td>59.296313</td>\n",
       "      <td>...</td>\n",
       "      <td>54.250629</td>\n",
       "      <td>67.574223</td>\n",
       "      <td>57.554642</td>\n",
       "      <td>69.739207</td>\n",
       "      <td>60.446863</td>\n",
       "      <td>71.757387</td>\n",
       "      <td>63.541532</td>\n",
       "      <td>73.292565</td>\n",
       "      <td>66.932465</td>\n",
       "      <td>3.456067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>164.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>177.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>177.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>246.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>245.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>247.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>248.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>249.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>249.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>247.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>244.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>305.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>305.000000</td>\n",
       "      <td>215.000000</td>\n",
       "      <td>305.000000</td>\n",
       "      <td>211.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>305.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>302.000000</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>361.000000</td>\n",
       "      <td>243.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>358.000000</td>\n",
       "      <td>231.000000</td>\n",
       "      <td>357.000000</td>\n",
       "      <td>225.000000</td>\n",
       "      <td>355.000000</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>209.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>213.000000</td>\n",
       "      <td>353.000000</td>\n",
       "      <td>217.000000</td>\n",
       "      <td>355.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>356.500000</td>\n",
       "      <td>228.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>435.000000</td>\n",
       "      <td>257.000000</td>\n",
       "      <td>432.000000</td>\n",
       "      <td>251.000000</td>\n",
       "      <td>428.000000</td>\n",
       "      <td>245.000000</td>\n",
       "      <td>425.000000</td>\n",
       "      <td>242.000000</td>\n",
       "      <td>425.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>251.000000</td>\n",
       "      <td>452.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>455.000000</td>\n",
       "      <td>258.000000</td>\n",
       "      <td>458.000000</td>\n",
       "      <td>261.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>265.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            col1_0       col1_1       col2_0       col2_1       col3_0  \\\n",
       "count  6463.000000  6463.000000  6463.000000  6463.000000  6463.000000   \n",
       "mean    301.635928   172.079375   301.376141   169.084326   301.932230   \n",
       "std      75.307941    70.621681    74.091861    67.475299    72.442872   \n",
       "min     164.000000    88.000000   170.000000    87.000000   176.000000   \n",
       "25%     246.000000   102.000000   245.000000   102.000000   247.000000   \n",
       "50%     305.000000   222.000000   305.000000   215.000000   305.000000   \n",
       "75%     361.000000   243.000000   360.000000   237.000000   358.000000   \n",
       "max     435.000000   257.000000   432.000000   251.000000   428.000000   \n",
       "\n",
       "            col3_1       col4_0       col4_1       col5_0       col5_1  ...  \\\n",
       "count  6463.000000  6463.000000  6463.000000  6463.000000  6463.000000  ...   \n",
       "mean    166.213523   301.843571   163.366084   301.711898   160.375058  ...   \n",
       "std      64.807385    71.027193    62.238422    69.215780    59.296313  ...   \n",
       "min      79.000000   177.000000    79.000000   177.000000    72.000000  ...   \n",
       "25%     102.000000   248.000000   104.000000   249.000000   106.000000  ...   \n",
       "50%     211.000000   304.000000   205.000000   305.000000   195.000000  ...   \n",
       "75%     231.000000   357.000000   225.000000   355.000000   220.000000  ...   \n",
       "max     245.000000   425.000000   242.000000   425.000000   236.000000  ...   \n",
       "\n",
       "           col41_1      col42_0      col42_1      col43_0      col43_1  \\\n",
       "count  6463.000000  6463.000000  6463.000000  6463.000000  6463.000000   \n",
       "mean    155.641343   299.240755   155.738357   299.663160   156.079684   \n",
       "std      54.250629    67.574223    57.554642    69.739207    60.446863   \n",
       "min      76.000000   155.000000    73.000000   152.000000    70.000000   \n",
       "25%     102.000000   249.000000    99.000000   247.000000    95.000000   \n",
       "50%     143.000000   301.000000   140.000000   302.000000   138.000000   \n",
       "75%     209.000000   351.000000   213.000000   353.000000   217.000000   \n",
       "max     251.000000   452.000000   254.000000   455.000000   258.000000   \n",
       "\n",
       "           col44_0      col44_1      col45_0      col45_1        col46  \n",
       "count  6463.000000  6463.000000  6463.000000  6463.000000  6463.000000  \n",
       "mean    299.417763   156.613337   299.710351   157.583475     6.844499  \n",
       "std      71.757387    63.541532    73.292565    66.932465     3.456067  \n",
       "min     146.000000    67.000000   143.000000    64.000000     2.000000  \n",
       "25%     246.000000    92.000000   244.000000    89.000000     4.000000  \n",
       "50%     303.000000   137.000000   303.000000   135.000000     7.000000  \n",
       "75%     355.000000   222.000000   356.500000   228.000000    10.000000  \n",
       "max     458.000000   261.000000   462.000000   265.000000    12.000000  \n",
       "\n",
       "[8 rows x 91 columns]"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('col46', axis=1).values\n",
    "y = df['col46'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  3.,  4.,  6.,  7.,  8., 10., 11., 12.])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_y = np.unique(y)\n",
    "output_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6463, 90)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.68170479,  0.84853915, -1.58432007, ..., -0.76512318,\n",
       "         1.24564767, -0.96497973],\n",
       "       [-1.66842495,  0.84853915, -1.48983544, ..., -0.93825169,\n",
       "         1.12284263, -0.84544702],\n",
       "       [-1.66842495,  0.90518336, -1.48983544, ..., -0.93825169,\n",
       "         1.13648763, -0.93509655],\n",
       "       ...,\n",
       "       [ 1.62497524,  1.01847178,  1.58766371, ..., -1.19007499,\n",
       "         0.03124227, -1.27875309],\n",
       "       [ 1.62497524,  1.07511599,  1.5471703 , ..., -1.26876977,\n",
       "        -0.02333775, -1.29369468],\n",
       "       [ 1.62497524,  1.07511599,  1.5336725 , ..., -1.19007499,\n",
       "         0.00395226, -1.23392832]])"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the data\n",
    "X = scaler.fit_transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 8, 8, 8], dtype=int64)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = encoder.fit_transform(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5170, 90)"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification model\n",
    "tf.random.set_seed(1234) # for consistent results\n",
    "model = models.Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape = 90),\n",
    "        layers.Dense(units=64, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "        layers.Dense(units=96, activation='relu'),\n",
    "        layers.Dense(units=len(output_y), activation='softmax')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_39 (Dense)            (None, 64)                5824      \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 96)                6240      \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 9)                 873       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,937\n",
      "Trainable params: 12,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 6, ..., 6, 7, 1], dtype=int64)"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "33/33 [==============================] - 1s 10ms/step - loss: 2.3943 - sparse_categorical_accuracy: 0.5109 - val_loss: 1.8205 - val_sparse_categorical_accuracy: 0.7447\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 11ms/step - loss: 1.4518 - sparse_categorical_accuracy: 0.8733 - val_loss: 1.1107 - val_sparse_categorical_accuracy: 0.9787\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.9340 - sparse_categorical_accuracy: 0.9855 - val_loss: 0.7784 - val_sparse_categorical_accuracy: 0.9971\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.6938 - sparse_categorical_accuracy: 0.9937 - val_loss: 0.6099 - val_sparse_categorical_accuracy: 0.9981\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.5554 - sparse_categorical_accuracy: 0.9973 - val_loss: 0.4970 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.4550 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.4078 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.3725 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3352 - val_sparse_categorical_accuracy: 0.9990\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.3073 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.2772 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.2548 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.2298 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.2125 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1927 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.1787 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1628 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.1520 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1396 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.1309 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1210 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.1140 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1062 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.1011 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0950 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.0906 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0850 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.0822 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0777 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.0745 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0707 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0686 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0657 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0644 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0614 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0601 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0577 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.0560 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0544 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.0532 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0510 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0505 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0487 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0478 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0466 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0455 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0439 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.0432 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0423 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0417 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0404 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.0399 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0388 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0386 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0374 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.0370 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0362 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0361 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0345 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0343 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0331 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.0326 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0318 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0318 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0329 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0311 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0305 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0299 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0291 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0288 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0290 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0279 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0273 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0270 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0262 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.0259 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0256 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0252 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0253 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0247 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0243 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0238 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0233 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.0231 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0228 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0226 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0221 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0220 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0218 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0213 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0210 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0208 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0203 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0202 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0203 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.0196 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0193 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0190 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0189 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0189 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0185 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0184 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0181 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0181 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0176 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0175 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0173 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0172 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0168 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0167 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0174 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0164 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0165 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0163 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0160 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0155 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0153 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0153 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0153 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0152 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0149 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0225 - sparse_categorical_accuracy: 0.9985 - val_loss: 0.0187 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.0167 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0150 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0146 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0141 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0139 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0137 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0141 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0147 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0176 - sparse_categorical_accuracy: 0.9985 - val_loss: 0.0254 - val_sparse_categorical_accuracy: 0.9990\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0275 - sparse_categorical_accuracy: 0.9978 - val_loss: 0.0213 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0233 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0187 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.0167 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0148 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0139 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0132 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0129 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0125 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0123 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0122 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.0120 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0121 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.0118 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0117 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0116 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0115 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.0113 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0114 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0112 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0111 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0110 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0108 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.0108 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0107 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0106 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0105 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0104 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0103 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0102 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0102 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0100 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0102 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0100 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0100 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.0098 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0099 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0097 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0096 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0095 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0095 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0093 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0092 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.0092 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0093 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.0091 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0091 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0090 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0092 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0089 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0087 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0087 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0088 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0085 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0086 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0084 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0084 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0083 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0083 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0083 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0083 - val_sparse_categorical_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "    loss=losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[metrics.SparseCategoricalAccuracy()]\n",
    ")\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 1ms/step - loss: 0.0072 - sparse_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.007187024690210819, 1.0]"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('saved_models/model_balle_2.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('dataset/new_pos_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
       "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
       "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
       "        7., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 12., 12.])"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = df_test.drop('col46', axis=1).values\n",
    "y_test = df_test['col46'].values\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.44126302,  1.73114769, -0.62021034, ...,  0.53877629,\n",
       "        -1.07149312,  0.51503057],\n",
       "       [-0.69122102, -0.82738597, -0.89924692, ..., -1.19970385,\n",
       "        -0.73968576, -1.1868736 ],\n",
       "       [-2.36835862, -0.74701318, -2.78049355, ...,  0.92510521,\n",
       "         0.22018553,  0.8932315 ],\n",
       "       ...,\n",
       "       [-0.58639992, -1.32971587,  0.5319407 , ...,  0.05253472,\n",
       "         0.196485  , -0.11747787],\n",
       "       [ 0.71176908, -1.45697278,  0.6669584 , ..., -0.83335746,\n",
       "         0.15500908, -0.82171409],\n",
       "       [-0.79604212, -1.63781155, -1.02526344, ...,  0.85183593,\n",
       "         0.01280593,  0.82150374]])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the data\n",
    "X_test = new_scaler.fit_transform(X_test)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 4, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 8, 8], dtype=int64)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = encoder.transform(y_test)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 4ms/step - loss: 7.8839 - sparse_categorical_accuracy: 0.3387\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7.8839111328125, 0.33870968222618103]"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002229C9BB310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.29962844e-04, 9.21872258e-01, 3.44104483e-05, 1.24796643e-04,\n",
       "        7.34079629e-02, 1.06857181e-06, 1.46981256e-05, 3.61481286e-03,\n",
       "        2.78951315e-08],\n",
       "       [9.16874111e-01, 9.55848955e-04, 1.09846434e-04, 1.90193805e-05,\n",
       "        6.89268654e-06, 3.71734679e-08, 8.20276961e-02, 6.42599571e-06,\n",
       "        7.67141657e-08],\n",
       "       [2.65381247e-01, 1.49067730e-01, 5.74992537e-01, 7.12324181e-05,\n",
       "        7.57114831e-05, 3.45727653e-06, 4.39867796e-03, 5.85540256e-05,\n",
       "        5.95085742e-03],\n",
       "       [1.11385293e-11, 5.17416862e-04, 1.98681297e-11, 2.67019956e-12,\n",
       "        1.52777266e-05, 5.52456483e-11, 4.39026098e-06, 9.99462903e-01,\n",
       "        3.39963124e-09],\n",
       "       [9.99999642e-01, 5.34421822e-08, 2.03390769e-07, 9.75922347e-08,\n",
       "        8.16526360e-12, 2.16088714e-15, 6.45401566e-10, 8.35699001e-18,\n",
       "        1.03912247e-13],\n",
       "       [8.75916034e-02, 1.42770051e-09, 4.03052419e-02, 3.37991696e-05,\n",
       "        6.39583109e-11, 6.98234317e-07, 8.71711671e-01, 2.39413711e-10,\n",
       "        3.56930657e-04],\n",
       "       [9.99999404e-01, 3.93845001e-09, 4.58197718e-07, 1.68792369e-07,\n",
       "        1.16214297e-12, 3.52103189e-15, 7.12494563e-10, 6.43267447e-19,\n",
       "        3.82903832e-13],\n",
       "       [9.99908686e-01, 2.63696398e-10, 9.12670803e-05, 3.91014277e-10,\n",
       "        2.28780228e-14, 8.81015096e-13, 8.89944474e-09, 1.57171316e-18,\n",
       "        7.87020663e-14],\n",
       "       [9.99999881e-01, 1.77510451e-09, 1.41752096e-07, 2.66533706e-09,\n",
       "        1.68572337e-13, 4.51516306e-17, 2.29610845e-13, 1.18936627e-21,\n",
       "        3.58717844e-16],\n",
       "       [1.41249448e-02, 1.11977480e-01, 8.73886049e-01, 3.48474920e-08,\n",
       "        8.55605413e-06, 2.90716957e-06, 5.21501642e-09, 7.57631184e-08,\n",
       "        9.05964903e-09],\n",
       "       [9.99963284e-01, 3.19009644e-07, 1.96607762e-05, 1.22625297e-05,\n",
       "        2.55480803e-10, 8.39055561e-12, 4.43355111e-06, 2.66071209e-14,\n",
       "        2.35081887e-09],\n",
       "       [1.04036163e-11, 2.13229010e-04, 8.84178755e-12, 6.34552350e-12,\n",
       "        1.81013693e-05, 2.75933651e-11, 4.92090112e-06, 9.99763787e-01,\n",
       "        2.01316963e-09],\n",
       "       [9.96710062e-01, 2.59962945e-07, 2.04065745e-03, 1.24886748e-03,\n",
       "        2.86158226e-08, 1.68541376e-08, 1.11367875e-07, 1.67387304e-13,\n",
       "        9.18268499e-12],\n",
       "       [9.82488513e-01, 7.84799681e-07, 1.58887036e-04, 1.73509251e-02,\n",
       "        7.72633996e-07, 6.71636480e-09, 7.05979608e-10, 1.80839095e-14,\n",
       "        7.24215372e-15],\n",
       "       [3.86203595e-07, 1.20106328e-04, 3.13401818e-01, 2.99049248e-07,\n",
       "        1.35925526e-04, 1.12809073e-02, 1.76857430e-02, 5.31070590e-01,\n",
       "        1.26304299e-01],\n",
       "       [6.34956232e-05, 8.75371873e-01, 4.68099643e-06, 9.63483581e-06,\n",
       "        1.15028761e-01, 4.69121005e-07, 2.99615158e-06, 9.51817818e-03,\n",
       "        1.21783001e-08],\n",
       "       [9.25620615e-01, 7.77202899e-07, 7.43296817e-02, 2.89673008e-05,\n",
       "        5.59875979e-10, 1.09803215e-08, 1.25976912e-05, 2.14561753e-13,\n",
       "        7.28874556e-06],\n",
       "       [8.54276610e-13, 8.02021459e-05, 1.52283203e-11, 9.13605888e-13,\n",
       "        9.97931511e-06, 1.09953859e-10, 1.92478069e-06, 9.99907851e-01,\n",
       "        7.08171566e-09],\n",
       "       [1.90384701e-01, 5.55538993e-09, 1.43778025e-05, 8.09600770e-01,\n",
       "        5.13096481e-08, 2.90967528e-10, 7.14285093e-08, 1.61673738e-14,\n",
       "        8.14058022e-14],\n",
       "       [3.36194468e-08, 5.97860105e-02, 6.36815400e-08, 8.45856329e-10,\n",
       "        2.27791665e-04, 5.91584248e-09, 9.84312428e-05, 9.39887226e-01,\n",
       "        4.18294036e-07],\n",
       "       [5.12227452e-05, 7.66219258e-01, 2.47705157e-06, 2.80365020e-05,\n",
       "        2.20222816e-01, 4.52876208e-07, 2.91665970e-06, 1.34728029e-02,\n",
       "        1.23288615e-08],\n",
       "       [9.85271692e-01, 1.99416681e-05, 1.47075010e-02, 3.41330775e-07,\n",
       "        4.29339408e-09, 2.46063592e-09, 4.61614405e-07, 1.52508770e-11,\n",
       "        4.33091313e-10],\n",
       "       [5.28848521e-10, 7.41799013e-04, 3.51241730e-10, 1.01789399e-09,\n",
       "        5.35034866e-04, 2.31153741e-09, 2.10609251e-05, 9.98702049e-01,\n",
       "        1.67473253e-08],\n",
       "       [1.78875620e-04, 3.63895595e-02, 2.13745279e-05, 2.66698725e-03,\n",
       "        9.60685551e-01, 5.38362747e-06, 6.62049686e-08, 5.22466580e-05,\n",
       "        4.12684470e-10],\n",
       "       [4.61386179e-07, 3.50431204e-01, 1.67101092e-07, 1.37932563e-08,\n",
       "        1.83736742e-03, 2.79725718e-08, 6.49377544e-05, 6.47665620e-01,\n",
       "        2.41573105e-07],\n",
       "       [4.94538363e-05, 5.14444113e-01, 4.82941478e-01, 1.10887113e-08,\n",
       "        2.51616584e-03, 3.94195376e-05, 3.80736137e-10, 2.23340089e-06,\n",
       "        7.10632139e-06],\n",
       "       [6.18141353e-01, 3.51445487e-06, 1.12477513e-02, 8.33182276e-05,\n",
       "        1.29417357e-08, 1.76803269e-07, 3.70506376e-01, 2.07381898e-08,\n",
       "        1.74617016e-05],\n",
       "       [4.03506428e-01, 3.77045684e-09, 2.29723076e-03, 5.92048228e-01,\n",
       "        7.85107568e-06, 2.80162676e-05, 2.09326390e-03, 1.24481148e-10,\n",
       "        1.90645369e-05],\n",
       "       [1.94477545e-09, 2.96895072e-04, 3.03010326e-02, 1.46320706e-10,\n",
       "        2.85456244e-05, 1.38094905e-03, 3.82833459e-05, 9.31261480e-01,\n",
       "        3.66928466e-02],\n",
       "       [9.98403370e-01, 5.74235104e-10, 1.57936371e-03, 1.72888322e-05,\n",
       "        2.59685832e-11, 1.26065991e-10, 1.38889513e-08, 1.03122729e-17,\n",
       "        1.40092815e-09],\n",
       "       [1.49552966e-06, 1.53679028e-02, 1.20368116e-01, 7.39762074e-09,\n",
       "        6.43275125e-05, 5.17014065e-04, 1.54345564e-03, 7.39014804e-01,\n",
       "        1.23122834e-01],\n",
       "       [9.99997854e-01, 2.00921363e-07, 1.88471063e-06, 5.16962828e-08,\n",
       "        1.84553622e-11, 1.18464071e-14, 1.26043065e-10, 9.93294113e-18,\n",
       "        2.94073034e-13],\n",
       "       [9.93240833e-01, 6.05652195e-09, 5.03598619e-03, 2.62514664e-06,\n",
       "        1.24494581e-11, 3.21532467e-09, 1.72061624e-03, 5.74962983e-13,\n",
       "        5.88826161e-08],\n",
       "       [1.84897636e-03, 9.97070909e-01, 9.19743684e-07, 1.30063745e-06,\n",
       "        4.50898049e-04, 7.63795194e-10, 1.90634692e-05, 6.07827154e-04,\n",
       "        6.14263085e-10],\n",
       "       [1.87164336e-01, 5.43624163e-01, 2.75644968e-04, 3.20997147e-04,\n",
       "        2.68588901e-01, 3.47164701e-07, 7.90816785e-07, 2.48668148e-05,\n",
       "        6.64488642e-09],\n",
       "       [8.21986362e-07, 9.91095662e-01, 1.16349752e-09, 9.02329877e-10,\n",
       "        4.15168033e-04, 4.86309734e-12, 2.45462644e-07, 8.48805346e-03,\n",
       "        7.70354180e-12],\n",
       "       [9.64900937e-11, 2.25914191e-04, 2.35235692e-10, 1.17625831e-07,\n",
       "        9.98756588e-02, 3.13059054e-08, 9.94483685e-07, 8.99897337e-01,\n",
       "        1.61548446e-08],\n",
       "       [1.09526932e-09, 1.22664242e-05, 9.78142396e-03, 1.22137146e-06,\n",
       "        4.99227643e-01, 4.90888476e-01, 1.18910257e-10, 3.94504459e-05,\n",
       "        4.94740161e-05],\n",
       "       [3.44517082e-01, 1.84715301e-08, 7.46026344e-05, 6.55383527e-01,\n",
       "        2.43456998e-05, 4.74113406e-08, 3.49123724e-07, 1.34943098e-11,\n",
       "        7.66735064e-10],\n",
       "       [2.94560523e-05, 1.49888173e-08, 5.16404659e-02, 6.46321103e-03,\n",
       "        2.53511939e-06, 1.05435342e-01, 5.30660152e-03, 7.76175966e-07,\n",
       "        8.31121564e-01],\n",
       "       [1.08322049e-04, 1.57876148e-08, 1.72136413e-06, 4.44788602e-05,\n",
       "        1.48028818e-07, 1.49774579e-07, 9.99834776e-01, 2.82779706e-06,\n",
       "        7.62753325e-06],\n",
       "       [2.65764322e-09, 3.60253443e-06, 1.42812217e-03, 8.25911629e-05,\n",
       "        2.76390702e-01, 6.99573815e-01, 1.86874740e-06, 1.37533592e-02,\n",
       "        8.76601040e-03],\n",
       "       [6.91050217e-02, 3.22032161e-02, 6.67037330e-06, 9.01180101e-05,\n",
       "        4.24255850e-05, 5.49227863e-09, 8.97110522e-01, 1.42035028e-03,\n",
       "        2.15564960e-05],\n",
       "       [2.00372132e-11, 4.12658374e-05, 2.39241717e-06, 2.07435091e-09,\n",
       "        2.97311548e-04, 2.22720468e-04, 3.93259006e-06, 9.97071266e-01,\n",
       "        2.36109062e-03],\n",
       "       [1.07944803e-03, 8.49915068e-07, 2.32481526e-07, 6.44050087e-05,\n",
       "        3.97265183e-07, 3.87092269e-09, 9.98840034e-01, 1.07552569e-05,\n",
       "        3.91284357e-06],\n",
       "       [9.86032248e-01, 4.42071876e-04, 3.72715804e-05, 1.91904415e-04,\n",
       "        4.73334865e-07, 1.56778357e-09, 1.32937124e-02, 3.99951681e-08,\n",
       "        2.26962516e-06],\n",
       "       [5.00948494e-03, 9.94878411e-01, 2.24291216e-06, 2.44995597e-08,\n",
       "        1.06923137e-04, 7.27252911e-11, 9.36111348e-08, 2.85899750e-06,\n",
       "        1.10276087e-10],\n",
       "       [2.32096535e-15, 1.56122593e-09, 7.38575636e-07, 9.64776348e-09,\n",
       "        4.35827449e-02, 9.51421916e-01, 3.49231422e-10, 4.81540198e-03,\n",
       "        1.79199313e-04],\n",
       "       [2.41346299e-09, 3.72153664e-07, 2.05147910e-04, 7.82081770e-06,\n",
       "        7.66543671e-05, 2.59223091e-03, 1.91804152e-02, 1.21588096e-01,\n",
       "        8.56349230e-01],\n",
       "       [5.55066615e-09, 1.84403942e-03, 3.56983332e-10, 1.99650851e-09,\n",
       "        4.76576941e-04, 2.98291780e-10, 4.72080574e-05, 9.97632027e-01,\n",
       "        5.97349583e-08],\n",
       "       [2.41346299e-09, 3.72153664e-07, 2.05147910e-04, 7.82081770e-06,\n",
       "        7.66543671e-05, 2.59223091e-03, 1.91804152e-02, 1.21588096e-01,\n",
       "        8.56349230e-01],\n",
       "       [3.01055564e-07, 1.50657340e-03, 5.77344338e-07, 2.68664223e-07,\n",
       "        6.89864828e-05, 1.30283297e-07, 2.47187205e-02, 9.73501146e-01,\n",
       "        2.03373551e-04],\n",
       "       [2.13182330e-01, 1.19876489e-07, 1.52690522e-02, 1.67673118e-02,\n",
       "        4.69966288e-08, 4.28990734e-06, 7.40050435e-01, 4.39812098e-09,\n",
       "        1.47262383e-02],\n",
       "       [2.35869015e-06, 9.89361584e-01, 7.65374608e-08, 6.73285538e-10,\n",
       "        1.40206728e-04, 2.00711225e-10, 3.59423461e-06, 1.04921153e-02,\n",
       "        5.89598281e-09],\n",
       "       [2.55598070e-14, 9.83769837e-07, 5.89867977e-06, 1.68610492e-09,\n",
       "        3.59180719e-02, 2.21188203e-01, 1.09115748e-08, 7.40236163e-01,\n",
       "        2.65066628e-03],\n",
       "       [1.44920932e-06, 3.37813725e-03, 1.59964655e-02, 2.53587746e-08,\n",
       "        1.82387230e-05, 8.53426245e-05, 2.17004940e-02, 6.61060393e-01,\n",
       "        2.97759444e-01],\n",
       "       [9.99822199e-01, 2.82700601e-08, 1.13414251e-04, 4.24623358e-05,\n",
       "        2.38083286e-11, 1.47250684e-11, 2.18704590e-05, 5.56506321e-15,\n",
       "        1.96678549e-08],\n",
       "       [1.49668442e-04, 2.00794756e-01, 4.01040126e-07, 1.05059798e-05,\n",
       "        8.88324715e-03, 3.75214668e-08, 5.91771817e-03, 7.84242928e-01,\n",
       "        7.56451470e-07],\n",
       "       [1.83625093e-06, 9.45134580e-01, 9.64145642e-03, 3.03216252e-09,\n",
       "        6.06582966e-04, 2.16549743e-05, 3.75978470e-06, 4.27225344e-02,\n",
       "        1.86758675e-03],\n",
       "       [4.88223698e-11, 4.53801127e-04, 1.91725530e-06, 1.09333043e-09,\n",
       "        9.85034159e-04, 1.51070330e-04, 7.37783239e-07, 9.98158872e-01,\n",
       "        2.48564349e-04],\n",
       "       [8.05477246e-17, 8.36195161e-15, 2.46571581e-04, 1.88588243e-15,\n",
       "        3.10603594e-14, 5.06923825e-05, 9.33724348e-11, 5.66443203e-09,\n",
       "        9.99702752e-01],\n",
       "       [1.11796171e-11, 1.18559973e-09, 1.45857011e-05, 5.44287815e-09,\n",
       "        1.47092889e-08, 1.30052586e-05, 5.23550960e-04, 1.37001497e-03,\n",
       "        9.98078823e-01]], dtype=float32)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict(X_test)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 0, 2, 7, 0, 6, 0, 0, 0, 2, 0, 7, 0, 0, 7, 1, 0, 7, 3, 7, 1, 0,\n",
       "        7, 4, 7, 1, 0, 3, 7, 0, 7, 0, 0, 1, 1, 1, 7, 4, 3, 8, 6, 5, 6, 7,\n",
       "        6, 0, 1, 5, 8, 7, 8, 7, 6, 1, 7, 7, 0, 7, 1, 7, 8, 8], dtype=int64),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 4, 6, 6, 6, 6,\n",
       "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 8, 8], dtype=int64))"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(prediction, axis=1)\n",
    "y_pred, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14,  4,  2,  2,  1,  0,  1,  9,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  3,  0,  1,  1,  0,  0,  1,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  1],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 2,  3,  0,  0,  0,  2,  4,  7,  2],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  2]], dtype=int64)"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.42      0.57        33\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.33      0.17      0.22         6\n",
      "           4       0.00      0.00      0.00         1\n",
      "           5       0.00      0.00      0.00         0\n",
      "           6       0.80      0.20      0.32        20\n",
      "           7       0.00      0.00      0.00         0\n",
      "           8       0.40      1.00      0.57         2\n",
      "\n",
      "    accuracy                           0.34        62\n",
      "   macro avg       0.27      0.20      0.19        62\n",
      "weighted avg       0.77      0.34      0.45        62\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bbbbb\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bbbbb\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bbbbb\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
