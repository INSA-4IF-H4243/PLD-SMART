{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from smart.model import NNModelBalle\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1_1</th>\n",
       "      <th>col1_2</th>\n",
       "      <th>col2_1</th>\n",
       "      <th>col2_2</th>\n",
       "      <th>col3_1</th>\n",
       "      <th>col3_2</th>\n",
       "      <th>col4_1</th>\n",
       "      <th>col4_2</th>\n",
       "      <th>col5_1</th>\n",
       "      <th>col5_2</th>\n",
       "      <th>...</th>\n",
       "      <th>col41_2</th>\n",
       "      <th>col42_1</th>\n",
       "      <th>col42_2</th>\n",
       "      <th>col43_1</th>\n",
       "      <th>col43_2</th>\n",
       "      <th>col44_1</th>\n",
       "      <th>col44_2</th>\n",
       "      <th>col45_1</th>\n",
       "      <th>col45_2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>210</td>\n",
       "      <td>316</td>\n",
       "      <td>217</td>\n",
       "      <td>285</td>\n",
       "      <td>232</td>\n",
       "      <td>281</td>\n",
       "      <td>230</td>\n",
       "      <td>295</td>\n",
       "      <td>246</td>\n",
       "      <td>264</td>\n",
       "      <td>...</td>\n",
       "      <td>133</td>\n",
       "      <td>497</td>\n",
       "      <td>103</td>\n",
       "      <td>519</td>\n",
       "      <td>110</td>\n",
       "      <td>514</td>\n",
       "      <td>96</td>\n",
       "      <td>526</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211</td>\n",
       "      <td>292</td>\n",
       "      <td>239</td>\n",
       "      <td>305</td>\n",
       "      <td>222</td>\n",
       "      <td>290</td>\n",
       "      <td>228</td>\n",
       "      <td>273</td>\n",
       "      <td>247</td>\n",
       "      <td>276</td>\n",
       "      <td>...</td>\n",
       "      <td>110</td>\n",
       "      <td>486</td>\n",
       "      <td>110</td>\n",
       "      <td>520</td>\n",
       "      <td>111</td>\n",
       "      <td>528</td>\n",
       "      <td>80</td>\n",
       "      <td>510</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>213</td>\n",
       "      <td>318</td>\n",
       "      <td>235</td>\n",
       "      <td>300</td>\n",
       "      <td>247</td>\n",
       "      <td>307</td>\n",
       "      <td>239</td>\n",
       "      <td>294</td>\n",
       "      <td>242</td>\n",
       "      <td>279</td>\n",
       "      <td>...</td>\n",
       "      <td>112</td>\n",
       "      <td>514</td>\n",
       "      <td>109</td>\n",
       "      <td>524</td>\n",
       "      <td>84</td>\n",
       "      <td>502</td>\n",
       "      <td>84</td>\n",
       "      <td>526</td>\n",
       "      <td>86</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>213</td>\n",
       "      <td>316</td>\n",
       "      <td>215</td>\n",
       "      <td>304</td>\n",
       "      <td>225</td>\n",
       "      <td>286</td>\n",
       "      <td>229</td>\n",
       "      <td>293</td>\n",
       "      <td>248</td>\n",
       "      <td>283</td>\n",
       "      <td>...</td>\n",
       "      <td>127</td>\n",
       "      <td>501</td>\n",
       "      <td>106</td>\n",
       "      <td>505</td>\n",
       "      <td>105</td>\n",
       "      <td>513</td>\n",
       "      <td>93</td>\n",
       "      <td>534</td>\n",
       "      <td>86</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>213</td>\n",
       "      <td>318</td>\n",
       "      <td>242</td>\n",
       "      <td>301</td>\n",
       "      <td>226</td>\n",
       "      <td>304</td>\n",
       "      <td>237</td>\n",
       "      <td>302</td>\n",
       "      <td>239</td>\n",
       "      <td>280</td>\n",
       "      <td>...</td>\n",
       "      <td>110</td>\n",
       "      <td>499</td>\n",
       "      <td>102</td>\n",
       "      <td>516</td>\n",
       "      <td>95</td>\n",
       "      <td>525</td>\n",
       "      <td>74</td>\n",
       "      <td>535</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3563</th>\n",
       "      <td>558</td>\n",
       "      <td>295</td>\n",
       "      <td>551</td>\n",
       "      <td>307</td>\n",
       "      <td>531</td>\n",
       "      <td>304</td>\n",
       "      <td>544</td>\n",
       "      <td>272</td>\n",
       "      <td>524</td>\n",
       "      <td>256</td>\n",
       "      <td>...</td>\n",
       "      <td>59</td>\n",
       "      <td>414</td>\n",
       "      <td>58</td>\n",
       "      <td>415</td>\n",
       "      <td>47</td>\n",
       "      <td>426</td>\n",
       "      <td>30</td>\n",
       "      <td>416</td>\n",
       "      <td>39</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3564</th>\n",
       "      <td>558</td>\n",
       "      <td>307</td>\n",
       "      <td>552</td>\n",
       "      <td>305</td>\n",
       "      <td>554</td>\n",
       "      <td>306</td>\n",
       "      <td>544</td>\n",
       "      <td>286</td>\n",
       "      <td>539</td>\n",
       "      <td>259</td>\n",
       "      <td>...</td>\n",
       "      <td>74</td>\n",
       "      <td>410</td>\n",
       "      <td>40</td>\n",
       "      <td>414</td>\n",
       "      <td>46</td>\n",
       "      <td>421</td>\n",
       "      <td>43</td>\n",
       "      <td>416</td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3565</th>\n",
       "      <td>558</td>\n",
       "      <td>310</td>\n",
       "      <td>542</td>\n",
       "      <td>289</td>\n",
       "      <td>530</td>\n",
       "      <td>290</td>\n",
       "      <td>532</td>\n",
       "      <td>282</td>\n",
       "      <td>520</td>\n",
       "      <td>279</td>\n",
       "      <td>...</td>\n",
       "      <td>74</td>\n",
       "      <td>418</td>\n",
       "      <td>55</td>\n",
       "      <td>418</td>\n",
       "      <td>47</td>\n",
       "      <td>417</td>\n",
       "      <td>43</td>\n",
       "      <td>405</td>\n",
       "      <td>29</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3566</th>\n",
       "      <td>559</td>\n",
       "      <td>317</td>\n",
       "      <td>529</td>\n",
       "      <td>312</td>\n",
       "      <td>536</td>\n",
       "      <td>290</td>\n",
       "      <td>527</td>\n",
       "      <td>283</td>\n",
       "      <td>535</td>\n",
       "      <td>259</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>423</td>\n",
       "      <td>47</td>\n",
       "      <td>412</td>\n",
       "      <td>33</td>\n",
       "      <td>412</td>\n",
       "      <td>54</td>\n",
       "      <td>408</td>\n",
       "      <td>38</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3567</th>\n",
       "      <td>559</td>\n",
       "      <td>316</td>\n",
       "      <td>558</td>\n",
       "      <td>307</td>\n",
       "      <td>538</td>\n",
       "      <td>280</td>\n",
       "      <td>545</td>\n",
       "      <td>288</td>\n",
       "      <td>544</td>\n",
       "      <td>276</td>\n",
       "      <td>...</td>\n",
       "      <td>62</td>\n",
       "      <td>417</td>\n",
       "      <td>45</td>\n",
       "      <td>423</td>\n",
       "      <td>33</td>\n",
       "      <td>404</td>\n",
       "      <td>28</td>\n",
       "      <td>422</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3568 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      col1_1  col1_2  col2_1  col2_2  col3_1  col3_2  col4_1  col4_2  col5_1  \\\n",
       "0        210     316     217     285     232     281     230     295     246   \n",
       "1        211     292     239     305     222     290     228     273     247   \n",
       "2        213     318     235     300     247     307     239     294     242   \n",
       "3        213     316     215     304     225     286     229     293     248   \n",
       "4        213     318     242     301     226     304     237     302     239   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "3563     558     295     551     307     531     304     544     272     524   \n",
       "3564     558     307     552     305     554     306     544     286     539   \n",
       "3565     558     310     542     289     530     290     532     282     520   \n",
       "3566     559     317     529     312     536     290     527     283     535   \n",
       "3567     559     316     558     307     538     280     545     288     544   \n",
       "\n",
       "      col5_2  ...  col41_2  col42_1  col42_2  col43_1  col43_2  col44_1  \\\n",
       "0        264  ...      133      497      103      519      110      514   \n",
       "1        276  ...      110      486      110      520      111      528   \n",
       "2        279  ...      112      514      109      524       84      502   \n",
       "3        283  ...      127      501      106      505      105      513   \n",
       "4        280  ...      110      499      102      516       95      525   \n",
       "...      ...  ...      ...      ...      ...      ...      ...      ...   \n",
       "3563     256  ...       59      414       58      415       47      426   \n",
       "3564     259  ...       74      410       40      414       46      421   \n",
       "3565     279  ...       74      418       55      418       47      417   \n",
       "3566     259  ...       60      423       47      412       33      412   \n",
       "3567     276  ...       62      417       45      423       33      404   \n",
       "\n",
       "      col44_2  col45_1  col45_2  label  \n",
       "0          96      526       67      1  \n",
       "1          80      510       80      1  \n",
       "2          84      526       86      1  \n",
       "3          93      534       86      1  \n",
       "4          74      535       90      1  \n",
       "...       ...      ...      ...    ...  \n",
       "3563       30      416       39     12  \n",
       "3564       43      416       31     12  \n",
       "3565       43      405       29     12  \n",
       "3566       54      408       38     12  \n",
       "3567       28      422       28     12  \n",
       "\n",
       "[3568 rows x 91 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset/datasetTrajectoire - Copy.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3568,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop(['label'], axis=1).values\n",
    "y = df['label'].values\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  3,  4,  6,  7,  8, 10, 11, 12], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Avant encoder y\n",
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    tf.keras.Input(shape = 90),\n",
    "    layers.Dense(units=64, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    layers.Dense(units=96, activation='relu'),\n",
    "    layers.Dense(units=9, activation='softmax')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                5824      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 96)                6240      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 9)                 873       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,937\n",
      "Trainable params: 12,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "model = NNModelBalle.load_model_from_layers(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 8, 8, 8], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = scaler.fit_transform(X)\n",
    "y = encoder.fit_transform(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_y = np.unique(y)\n",
    "output_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = model.split_train_test(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "23/23 [==============================] - 1s 3ms/step - loss: 2.9643 - accuracy: 0.2106\n",
      "Epoch 2/80\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2.7869 - accuracy: 0.2561\n",
      "Epoch 3/80\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2.6419 - accuracy: 0.2971\n",
      "Epoch 4/80\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 2.5191 - accuracy: 0.3560\n",
      "Epoch 5/80\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2.4124 - accuracy: 0.4467\n",
      "Epoch 6/80\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 2.3169 - accuracy: 0.5130\n",
      "Epoch 7/80\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 2.2287 - accuracy: 0.5459\n",
      "Epoch 8/80\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 2.1479 - accuracy: 0.5939\n",
      "Epoch 9/80\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 2.0723 - accuracy: 0.6324\n",
      "Epoch 10/80\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2.0004 - accuracy: 0.6692\n",
      "Epoch 11/80\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1.9321 - accuracy: 0.6889\n",
      "Epoch 12/80\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.8670 - accuracy: 0.6987\n",
      "Epoch 13/80\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.8045 - accuracy: 0.7165\n",
      "Epoch 14/80\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.7448 - accuracy: 0.7400\n",
      "Epoch 15/80\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1.6869 - accuracy: 0.7586\n",
      "Epoch 16/80\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.6304 - accuracy: 0.7786\n",
      "Epoch 17/80\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.5758 - accuracy: 0.7950\n",
      "Epoch 18/80\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1.5246 - accuracy: 0.8255\n",
      "Epoch 19/80\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.4751 - accuracy: 0.8406\n",
      "Epoch 20/80\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.4281 - accuracy: 0.8588\n",
      "Epoch 21/80\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3837 - accuracy: 0.8742\n",
      "Epoch 22/80\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.3407 - accuracy: 0.8858\n",
      "Epoch 23/80\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.2993 - accuracy: 0.8900\n",
      "Epoch 24/80\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.2600 - accuracy: 0.9068\n",
      "Epoch 25/80\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.2224 - accuracy: 0.9222\n",
      "Epoch 26/80\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.1870 - accuracy: 0.9282\n",
      "Epoch 27/80\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.1537 - accuracy: 0.9345\n",
      "Epoch 28/80\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.1220 - accuracy: 0.9439\n",
      "Epoch 29/80\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1.0922 - accuracy: 0.9481\n",
      "Epoch 30/80\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.0639 - accuracy: 0.9530\n",
      "Epoch 31/80\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.0368 - accuracy: 0.9548\n",
      "Epoch 32/80\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1.0110 - accuracy: 0.9583\n",
      "Epoch 33/80\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.9862 - accuracy: 0.9625\n",
      "Epoch 34/80\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.9621 - accuracy: 0.9615\n",
      "Epoch 35/80\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.9395 - accuracy: 0.9646\n",
      "Epoch 36/80\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.9180 - accuracy: 0.9650\n",
      "Epoch 37/80\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.8973 - accuracy: 0.9671\n",
      "Epoch 38/80\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.8776 - accuracy: 0.9678\n",
      "Epoch 39/80\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.8589 - accuracy: 0.9695\n",
      "Epoch 40/80\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.8408 - accuracy: 0.9692\n",
      "Epoch 41/80\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.8242 - accuracy: 0.9688\n",
      "Epoch 42/80\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.8076 - accuracy: 0.9699\n",
      "Epoch 43/80\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.7915 - accuracy: 0.9741\n",
      "Epoch 44/80\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7764 - accuracy: 0.9737\n",
      "Epoch 45/80\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7619 - accuracy: 0.9737\n",
      "Epoch 46/80\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.7478 - accuracy: 0.9741\n",
      "Epoch 47/80\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.7341 - accuracy: 0.9737\n",
      "Epoch 48/80\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.7209 - accuracy: 0.9772\n",
      "Epoch 49/80\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7080 - accuracy: 0.9769\n",
      "Epoch 50/80\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6958 - accuracy: 0.9786\n",
      "Epoch 51/80\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.6840 - accuracy: 0.9800\n",
      "Epoch 52/80\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.6723 - accuracy: 0.9779\n",
      "Epoch 53/80\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.6611 - accuracy: 0.9800\n",
      "Epoch 54/80\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.6501 - accuracy: 0.9814\n",
      "Epoch 55/80\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.6394 - accuracy: 0.9846\n",
      "Epoch 56/80\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.6287 - accuracy: 0.9881\n",
      "Epoch 57/80\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.6188 - accuracy: 0.9884\n",
      "Epoch 58/80\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.6084 - accuracy: 0.9895\n",
      "Epoch 59/80\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.5991 - accuracy: 0.9842\n",
      "Epoch 60/80\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.5892 - accuracy: 0.9895\n",
      "Epoch 61/80\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.5798 - accuracy: 0.9923\n",
      "Epoch 62/80\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.5705 - accuracy: 0.9916\n",
      "Epoch 63/80\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.5616 - accuracy: 0.9926\n",
      "Epoch 64/80\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.5527 - accuracy: 0.9933\n",
      "Epoch 65/80\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.5437 - accuracy: 0.9940\n",
      "Epoch 66/80\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.5351 - accuracy: 0.9951\n",
      "Epoch 67/80\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.5267 - accuracy: 0.9954\n",
      "Epoch 68/80\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.5184 - accuracy: 0.9968\n",
      "Epoch 69/80\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.5101 - accuracy: 0.9975\n",
      "Epoch 70/80\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.5022 - accuracy: 0.9968\n",
      "Epoch 71/80\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4943 - accuracy: 0.9972\n",
      "Epoch 72/80\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.4863 - accuracy: 0.9986\n",
      "Epoch 73/80\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4787 - accuracy: 0.9986\n",
      "Epoch 74/80\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4713 - accuracy: 0.9975\n",
      "Epoch 75/80\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.4640 - accuracy: 0.9975\n",
      "Epoch 76/80\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4568 - accuracy: 0.9979\n",
      "Epoch 77/80\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4494 - accuracy: 0.9989\n",
      "Epoch 78/80\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4422 - accuracy: 0.9986\n",
      "Epoch 79/80\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4352 - accuracy: 0.9996\n",
      "Epoch 80/80\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.4283 - accuracy: 0.9996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2bec9125fd0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.train(X_train, y_train, epochs=80, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1_1</th>\n",
       "      <th>col1_2</th>\n",
       "      <th>col2_1</th>\n",
       "      <th>col2_2</th>\n",
       "      <th>col3_1</th>\n",
       "      <th>col3_2</th>\n",
       "      <th>col4_1</th>\n",
       "      <th>col4_2</th>\n",
       "      <th>col5_1</th>\n",
       "      <th>col5_2</th>\n",
       "      <th>...</th>\n",
       "      <th>col41_2</th>\n",
       "      <th>col42_1</th>\n",
       "      <th>col42_2</th>\n",
       "      <th>col43_1</th>\n",
       "      <th>col43_2</th>\n",
       "      <th>col44_1</th>\n",
       "      <th>col44_2</th>\n",
       "      <th>col45_1</th>\n",
       "      <th>col45_2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>238.5</td>\n",
       "      <td>31.5</td>\n",
       "      <td>244.5</td>\n",
       "      <td>39.5</td>\n",
       "      <td>227.5</td>\n",
       "      <td>38.5</td>\n",
       "      <td>246.5</td>\n",
       "      <td>42.5</td>\n",
       "      <td>238.5</td>\n",
       "      <td>46.0</td>\n",
       "      <td>...</td>\n",
       "      <td>69.0</td>\n",
       "      <td>384.5</td>\n",
       "      <td>78.0</td>\n",
       "      <td>372.5</td>\n",
       "      <td>69.0</td>\n",
       "      <td>379.5</td>\n",
       "      <td>66.0</td>\n",
       "      <td>373.5</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>237.5</td>\n",
       "      <td>36.5</td>\n",
       "      <td>232.5</td>\n",
       "      <td>31.5</td>\n",
       "      <td>239.5</td>\n",
       "      <td>34.5</td>\n",
       "      <td>236.5</td>\n",
       "      <td>28.5</td>\n",
       "      <td>227.5</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>62.0</td>\n",
       "      <td>375.5</td>\n",
       "      <td>77.0</td>\n",
       "      <td>376.5</td>\n",
       "      <td>69.0</td>\n",
       "      <td>375.5</td>\n",
       "      <td>75.0</td>\n",
       "      <td>373.5</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>239.5</td>\n",
       "      <td>39.5</td>\n",
       "      <td>238.5</td>\n",
       "      <td>39.5</td>\n",
       "      <td>246.5</td>\n",
       "      <td>24.5</td>\n",
       "      <td>245.5</td>\n",
       "      <td>34.5</td>\n",
       "      <td>236.5</td>\n",
       "      <td>33.0</td>\n",
       "      <td>...</td>\n",
       "      <td>75.0</td>\n",
       "      <td>366.5</td>\n",
       "      <td>67.0</td>\n",
       "      <td>383.5</td>\n",
       "      <td>72.0</td>\n",
       "      <td>366.5</td>\n",
       "      <td>77.0</td>\n",
       "      <td>372.5</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>238.5</td>\n",
       "      <td>33.5</td>\n",
       "      <td>237.5</td>\n",
       "      <td>30.5</td>\n",
       "      <td>240.5</td>\n",
       "      <td>41.5</td>\n",
       "      <td>232.5</td>\n",
       "      <td>40.5</td>\n",
       "      <td>222.5</td>\n",
       "      <td>43.0</td>\n",
       "      <td>...</td>\n",
       "      <td>74.0</td>\n",
       "      <td>381.5</td>\n",
       "      <td>78.0</td>\n",
       "      <td>373.5</td>\n",
       "      <td>60.0</td>\n",
       "      <td>366.5</td>\n",
       "      <td>61.0</td>\n",
       "      <td>369.5</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>228.5</td>\n",
       "      <td>41.5</td>\n",
       "      <td>235.5</td>\n",
       "      <td>26.5</td>\n",
       "      <td>239.5</td>\n",
       "      <td>38.5</td>\n",
       "      <td>242.5</td>\n",
       "      <td>39.5</td>\n",
       "      <td>223.5</td>\n",
       "      <td>43.0</td>\n",
       "      <td>...</td>\n",
       "      <td>77.0</td>\n",
       "      <td>367.5</td>\n",
       "      <td>66.0</td>\n",
       "      <td>382.5</td>\n",
       "      <td>63.0</td>\n",
       "      <td>369.5</td>\n",
       "      <td>60.0</td>\n",
       "      <td>376.5</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>253.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>...</td>\n",
       "      <td>73.0</td>\n",
       "      <td>346.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>344.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>341.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>246.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>274.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>...</td>\n",
       "      <td>73.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>328.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>337.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>242.0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>...</td>\n",
       "      <td>56.0</td>\n",
       "      <td>342.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>342.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>339.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>248.0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>...</td>\n",
       "      <td>65.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>338.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>344.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>335.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>260.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>281.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>281.0</td>\n",
       "      <td>...</td>\n",
       "      <td>58.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>344.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>346.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>334.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>705 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     col1_1  col1_2  col2_1  col2_2  col3_1  col3_2  col4_1  col4_2  col5_1  \\\n",
       "0     238.5    31.5   244.5    39.5   227.5    38.5   246.5    42.5   238.5   \n",
       "1     237.5    36.5   232.5    31.5   239.5    34.5   236.5    28.5   227.5   \n",
       "2     239.5    39.5   238.5    39.5   246.5    24.5   245.5    34.5   236.5   \n",
       "3     238.5    33.5   237.5    30.5   240.5    41.5   232.5    40.5   222.5   \n",
       "4     228.5    41.5   235.5    26.5   239.5    38.5   242.5    39.5   223.5   \n",
       "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "700   253.0   270.0   260.0   273.0   243.0   286.0   255.0   278.0   250.0   \n",
       "701   246.0   270.0   243.0   274.0   257.0   284.0   247.0   289.0   252.0   \n",
       "702   242.0   283.0   243.0   282.0   254.0   289.0   249.0   287.0   255.0   \n",
       "703   248.0   285.0   256.0   280.0   246.0   278.0   261.0   275.0   242.0   \n",
       "704   260.0   275.0   242.0   281.0   252.0   282.0   259.0   284.0   257.0   \n",
       "\n",
       "     col5_2  ...  col41_2  col42_1  col42_2  col43_1  col43_2  col44_1  \\\n",
       "0      46.0  ...     69.0    384.5     78.0    372.5     69.0    379.5   \n",
       "1      32.0  ...     62.0    375.5     77.0    376.5     69.0    375.5   \n",
       "2      33.0  ...     75.0    366.5     67.0    383.5     72.0    366.5   \n",
       "3      43.0  ...     74.0    381.5     78.0    373.5     60.0    366.5   \n",
       "4      43.0  ...     77.0    367.5     66.0    382.5     63.0    369.5   \n",
       "..      ...  ...      ...      ...      ...      ...      ...      ...   \n",
       "700   289.0  ...     73.0    346.0     66.0    344.0     71.0    332.0   \n",
       "701   289.0  ...     73.0    340.0     63.0    330.0     70.0    328.0   \n",
       "702   277.0  ...     56.0    342.0     62.0    331.0     69.0    342.0   \n",
       "703   284.0  ...     65.0    331.0     67.0    338.0     60.0    344.0   \n",
       "704   281.0  ...     58.0    343.0     74.0    344.0     70.0    346.0   \n",
       "\n",
       "     col44_2  col45_1  col45_2  label  \n",
       "0       66.0    373.5     76.0      1  \n",
       "1       75.0    373.5     73.0      1  \n",
       "2       77.0    372.5     76.0      1  \n",
       "3       61.0    369.5     73.0      1  \n",
       "4       60.0    376.5     78.0      1  \n",
       "..       ...      ...      ...    ...  \n",
       "700     67.0    341.0     56.0     10  \n",
       "701     69.0    337.0     73.0     10  \n",
       "702     66.0    339.0     65.0     10  \n",
       "703     67.0    335.0     68.0     10  \n",
       "704     62.0    334.0     74.0     10  \n",
       "\n",
       "[705 rows x 91 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('dataset/datasetTest - Copy.csv')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(705,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_fin = test_df.drop(['label'], axis=1).values\n",
    "y_test_fin = test_df['label'].values\n",
    "y_test_fin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  6, 10], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Avant encoder y_test_fin\n",
    "np.unique(y_test_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_fin = new_scaler.fit_transform(X_test_fin)\n",
    "y_test_fin = encoder.transform(y_test_fin)\n",
    "y_test_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 6], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apres encoder y_test_fin\n",
    "output_y_test_fin = np.unique(y_test_fin)\n",
    "output_y_test_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 2ms/step - loss: 5.9263 - accuracy: 0.1901\n",
      "Loss:  5.926254749298096\n",
      "Accuracy:  0.19007092714309692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.926254749298096, 0.19007092714309692]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Evaluate the model\n",
    "score = model.evaluate(X_test_fin, y_test_fin)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_model('saved_models/NeuralNetworkModel_balle.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Predict\n",
    "y_pred = model.predict(X_test_fin)\n",
    "y_pred, y_test_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "confusion_matrix(y_test_fin, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_fin, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
